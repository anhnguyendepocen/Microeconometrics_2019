[
["session-ii-maximum-likelihood-estimation-mle.html", "Chapter 3 Session II - Maximum Likelihood Estimation (MLE) 3.1 Introduction 3.2 Maximum likelihood estimator 3.3 Example II - Structural estimation 3.4 References", " Chapter 3 Session II - Maximum Likelihood Estimation (MLE) 3.1 Introduction In this second session of the microeconometrics tutorial we are going to use R and it`s functionality to do maximum likelihood estimation. The task that we are going to complete in this section are: Understand the intuition behind Maximum likelihood estimation. We are going to replicate a Poisson regression table (REF) using MLE. Estimates are performed using a programmed function and a build-in function. Propose a model and derive the likelihood function from it. This part is not going to be very deep in the explanation of the model, derivation and assumptions. The aim of this sessions are on the estimation (computation) and not in the model per se. I will however referentiate the sources if you want to have a deeper look at the model. Given that the last session we did something on health economics, this time we change topic and will focus on labour economics. As anecdote, when I first saw this I was very impressed! I hope you are impressed too after today session! Use the Current Population Survey (CPS) and understand how to handle and manage this particular data set. + Import the data + Use the specified columns + Clean data Estimate the structural parameters of the proposed model (both for the estimates and the standard errors, estimated using the delta method). 3.2 Maximum likelihood estimator For the theory please check the slides of the course (class 2). Here we provide just a brief definition and the intuition on the application of the method. What is Maximum likelihood estimation (MLE)? MLE is a method of estimation in which we obtain the parameters of our model under an assumed statistical model and the available data, such that our sample is the most probable. Given a statistical model (given a model), select the parameters that make the observed data more probable. By so we are doing inference in the population that generated our data. and the DGP behind. We can formulate any model and we will obtain a result; the only restriction for the formulation is that it has probability 0. Even if it is intuitive, rely on the assumptions (model, statistical model, DGP), but not in the validity. Validity of the models? “A model is a deliberate abstraction from reality. One model can be better than another, on one or several dimensions, but none are correct. They help us focus on the small set of phenomena in which we are interested, and/or have data regarding. When correctly developed and explained, it should be clear what set of phenomena are being excluded from consideration, and, at the end of the analysis, it is desirable to say how the omission of other relevant phenomena could have affected the results attained. An advantage of a structured approach to empirical analysis is that it should be immediately clear what factors have been considered exogenous and which endogenous, the functional form assumptions made, etc.&quot; (C. Flinn, lecture notes) In order to understand how MLE works we are going to make two examples today: Poisson regresson and a structural estimation. 3.2.1 Poisson regression In this section we are going to replicate a paper by Daniel Treisman published in AER 2016. that uses the Poisson regression to modelate the number of millioners in a set of countries, given some country characteristics (Treisman 2016). What does the paper says? The main idea of the paper is provide a robust model from which we can predict the number of rich in a given country, given an economic environment (so called, country characteristics). After that is possible to compare the prediction versus the data; the author realizes that and no matter how the model is setted and specified, Russia has the largest number of anomalies (underpredictions). Why a Poisson regression? In Treisman’s paper, the dependent variable — the number of billionaires \\(y_i\\) in country \\(i\\) — is modeled as a function of GDP per capita, population size, and years membership in GATT and WTO. He present 4 specifications including more variables. “… since the dependent variable is a count, Poisson rather than OLS regression is appropriate.” 3.2.1.1 Estimation You can acces the data and the paper in the provided links. Download and unzip the information. The file also contains a companion STATA code to reproduce the tables in the paper. Unzip the information and charge the dataset into R using the haven library (Wickham and Miller 2018): data_mil &lt;- read_dta(&quot;YOUR PATH GOES HERE&quot;) To reproduce table 1 from the paper we need to filter the information for 2008. will only use that info for reproducing the table. data_mil &lt;- data_mil[data_mil$year == 2008,] Our goal is to estimate a Poisson regression model, and there are built-in functions to do these kind of estimations using a command like glm(..., family = &quot;poisson&quot;). Our goal instead is to use Maximum Likelihood estimation to reproduce such parameters and understand how does this work. In order to have a benchmark for comparison let’s see how the output of the first proposed model looks like: summary(glm(formula = numbil0 ~ lngdppc + lnpop + gattwto08, family = poisson(link=&#39;log&#39;), data = data_mil)) ## ## Call: ## glm(formula = numbil0 ~ lngdppc + lnpop + gattwto08, family = poisson(link = &quot;log&quot;), ## data = data_mil) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -8.7615 -0.7585 -0.3775 -0.1010 9.0587 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -29.049536 0.638193 -45.518 &lt; 2e-16 *** ## lngdppc 1.083856 0.035064 30.911 &lt; 2e-16 *** ## lnpop 1.171362 0.024156 48.491 &lt; 2e-16 *** ## gattwto08 0.005968 0.001908 3.127 0.00176 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 5942.23 on 196 degrees of freedom ## Residual deviance: 669.95 on 193 degrees of freedom ## (354 observations deleted due to missingness) ## AIC: 885.08 ## ## Number of Fisher Scoring iterations: 5 Let’s start our construction of the maximum likelihood introducing the Poisson regression model. Poisson regression is a generalized linear model form of regression analysis used to model count data \\((1)\\), in which the dependent variable have a Poisson distribution \\((2)\\), and the logarithm of the expected value can be modeled as a linear combination \\((3)\\). Count data: remark that all the numbers from a Poisson distribution are integers. Poisson distribution is defined as: \\[f(k,\\lambda)=Pr(X=k)=\\frac{\\lambda^k e^{-\\lambda}}{k!}\\] A remark is that the mean and variance form the Posson distribution is equal to a constant \\(\\lambda\\). The mean can be expressed as a linear combination of the parameters. \\[ \\mu = E[y] = E[e^{y}] = E(y| \\boldsymbol{X})=e^{\\theta^{\\prime} \\boldsymbol{X}}=e^{\\theta_0 + \\theta_1 x_{i1}+ ... + \\theta_k x_{ik} }\\] Combining condition \\((2)\\) and \\((3)\\), we obtain that the probability mass function is given by: \\[f(y_i,\\lambda)=f(y_i,\\mu)=\\frac{\\mu^y_i e^{-\\mu}}{y_i!}\\] The joint probability mass function is the equal to: \\[f(\\boldsymbol{Y}|\\boldsymbol{X},\\boldsymbol{\\theta})=f(y_1,\\mu) f(y_2,\\mu)...f(y_n,\\mu) = \\prod_{i=1}^n f(y_i,\\mu|\\boldsymbol{X},\\boldsymbol{\\theta})\\] Our likelihood function instead takes as given the data (vector \\(\\boldsymbol{Y}\\) and matrix \\(\\boldsymbol{X}\\)) and calculates the likelihood given a parameter \\(\\theta\\). \\[\\mathcal{L}(\\boldsymbol{\\theta} | \\boldsymbol{Y},\\boldsymbol{X}) = \\prod_{i=1}^n f(\\boldsymbol{\\theta}|\\boldsymbol{X},\\boldsymbol{Y}) = \\prod_{i=1}^n \\frac{\\mu_i^{y_i} e^{-\\mu_i}}{y_i!}\\] Finding the \\(\\boldsymbol{\\hat{\\theta}}\\) that maximizes the likelihood function retrieves the estimation parameters. Before doing so we are going to use a monotonic transformation and take use as objective function the log-likelihhod of the function. Why? Think about derivatives of sums vs. derivatives of products. \\[\\boldsymbol{\\hat{\\theta}} = \\max_{\\boldsymbol{\\theta}} log( \\mathcal{L}(\\boldsymbol{\\theta} | \\boldsymbol{Y},\\boldsymbol{X})) = \\min _{\\boldsymbol{\\theta}} - log( \\mathcal{L}(\\boldsymbol{\\theta} | \\boldsymbol{Y},\\boldsymbol{X})) \\] Taking the logarithm then we have: \\[ \\begin{equation} \\begin{aligned} \\log( \\mathcal{L}(\\boldsymbol{\\theta} | \\boldsymbol{Y},\\boldsymbol{X})) &amp;= \\log \\left( \\prod_{i=1}^N \\frac{\\mu_i^y e^{-\\mu}}{y_i!} \\right) \\\\ &amp;= \\sum_{i=1}^N \\log \\left( \\frac{\\mu_i^{y_i} e^{-\\mu_i}}{y_i!} \\right)\\\\ &amp;= \\sum_{i=1}^N y_i \\log \\left(\\mu_i \\right) - \\sum_{i=1}^N \\log \\left(\\mu_i\\right) -\\sum_{i=1}^N \\log \\left(y_i! \\right)\\\\ \\end{aligned} \\end{equation} \\] Where, \\[ \\mu = e^{\\theta^{\\prime} \\boldsymbol{X}}=e^{\\theta_0 + \\theta_1 x_{i1}+ ... + \\theta_k x_{ik} }\\]. Given that we can not find a solution using algebra, we are going to use the optimzer from the statistical software to maximize the log-likelihood function above derived. To do so we are going to use the optim function from R. After going into the help vignette ?optim, we realize we are going to need: A vector of initial values The function to be minimized, that receives as input the vector of parameters and should return a scalar so it can be optimized. Method of optimization (?) Optional hessian (boolean). Given that we need to calculate the variance covariance matrix to estimate the standard errors of he parameters it’s going to come in handy the resulted Heassian. Last tutorial we introduce the elements of a function, and the structure to define a funcion in R. In this tutorial we just recall as good practice that we are going to differenciate the inpus of the function: the parameters are the inputs that each timeare going to change in the optimization process, while the data for example will be a static kind of input. Another static input that we are going to use is the formula, a type of R object which will allow us to extract the relevant information of he name of the variables, the columns, and the relationship between the variables (dependent or independent, interactions, dependance,…). We start by writing the function. We call the function ‘LLcalc’, and define the inputs in the parentesis. One usefull function that we use in the first line is model.matrix(). This function takes a formula and provide the matrix that we are using in the regression, including the vector of ones of the intercept, the dummy variables, and the interactions. We are going to calculate the value of \\(\\mu\\), and then the value of the individual contribution to the log-likelihood. After that we just sum and put a negative, since the optimizer minimizes by default. There is one technical detail that we are adressing using the package Rmpfr, which allow to store big in 128 bits (as the factorail of 400) (Maechler 2019). LLcalc &lt;- function(theta, formula, data){ ### Calculate the log-likelihood of a Poisson regression, ### given a vector of parameters (1), ### a relationship ()formula (2) ### and a dataset (type: dataframe) rhs &lt;- model.matrix(formula, data = data) colnames(rhs)[1] &lt;- &quot;Intercept&quot; Y &lt;- as.matrix(data[rownames(rhs),toString(formula[[2]])]) # Expected values \\mu mu &lt;- exp(rhs %*% theta) # Value of the log likelihood LL &lt;- Y * log(mu)-mu-as.numeric(log(gamma(as(Y+1,&quot;mpfr&quot;)))) #print(cbind(colnames(rhs),round(theta,3))) return(-sum(LL, na.rm = T)) } In order to test the function we are going to test it with the data, a random \\(\\theta\\), and the formula of the first column of table I in the paper. theta_test &lt;- rnorm(4) formula_1 &lt;- as.formula(numbil0 ~ lngdppc + lnpop + gattwto08) LLcalc(theta = theta_test, formula = formula_1, data = data_mil) ## [1] 51011.57 Now that we know is working is time to set up the optimizer. As we discuss before we need the function, some starting parameters, and the data and the formula which are the static inputs of our function. theta_0 &lt;- c(-30,1,1,0) dTbp_beta &lt;- optim(par = theta_0, fn = LLcalc, data = data_mil, formula = formula_1, method = &quot;Nelder-Mead&quot;, hessian = TRUE) round(dTbp_beta$par,3) ## [1] -29.052 1.084 1.171 0.006 In order to complete the estimation we need calculate the standard errors. Since we have the Hessian, from we theory of the course we know that we can recover the Standard Errors, because: The negative of the hessian (provided as a linearization arround the optimum of the LL maximization) is equal to the Fisher information matrix. \\[[\\mathcal{I}(\\theta)]_{i,j}= \\mathbf{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta_i} \\log f(X|\\theta)\\right)\\left(\\frac{\\partial}{\\partial \\theta_j} \\log f(X|\\theta)\\right)|\\theta\\right]=\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log f(X|\\theta)|\\theta\\right]\\] - The Fisher information matrix, when inverted is equal to the variance covariance matrix. (Actually, formally what Cramer Rao states is that the inverse is the lower bound of the variance if the estimator is unbiased.) - The variance covariance matrix has in the diagonal the variane for each parameter. Taking square root of it is going to give the standard errors. - Given that we minimize we dont have to multiply by -1, since the values of the LL are calculated over the negative likelihhod function. Taking in consideration these notions we the following standard errors that are equal to the previous regression. fisher_info &lt;- dTbp_beta$hessian vcov &lt;- solve(fisher_info) se &lt;- sqrt(diag(vcov)) se ## [1] 0.638233253 0.035068731 0.024155062 0.001907374 To complete the exercise we are going to reproduce the whole table 1 form the paper. For that we set up all the formulas (models) estimated. The paer provide the robust herrors, so we calculate them and we put them in the stargazer (Hlavac 2018) output. formula_2 &lt;- as.formula(numbil0 ~ lngdppc + lnpop + gattwto08 + lnmcap08 + rintr + topint08) formula_3 &lt;- as.formula(numbil0 ~ lngdppc + lnpop + gattwto08 + lnmcap08 + rintr + topint08 + nrrents + roflaw) formula_4 &lt;- as.formula(numbil0 ~ lngdppc + lnpop + gattwto08 + lnmcap08 + rintr + topint08 + nrrents + roflaw + fullprivproc) r1 &lt;- glm(formula = formula_1, family = poisson(link=&#39;log&#39;), data = data_mil) r2 &lt;- glm(formula = formula_2, family = poisson(link=&#39;log&#39;), data = data_mil) r3 &lt;- glm(formula = formula_3, family = poisson(link=&#39;log&#39;), data = data_mil) r4 &lt;- glm(formula = formula_4, family = poisson(link=&#39;log&#39;), data = data_mil) se_rob &lt;- list(sqrt(diag(sandwich::vcovHC.default(r1,type = &quot;HC0&quot;))), sqrt(diag(sandwich::vcovHC.default(r2,type = &quot;HC0&quot;))), sqrt(diag(sandwich::vcovHC.default(r3,type = &quot;HC0&quot;))), sqrt(diag(sandwich::vcovHC.default(r4,type = &quot;HC0&quot;)))) stargazer::stargazer(r1,r2,r3,r4, title = &quot;Table 1 - Poisson regression&quot;, type=ifelse(knitr::is_latex_output(),&quot;latex&quot;,&quot;html&quot;), se = se_rob, out = &quot;./images/RB_T_I.tex&quot;) (#tab:)Table 1 - Poisson regression Dependent variable: numbil0 (1) (2) (3) (4) lngdppc 1.084*** 0.717*** 0.737*** 0.963*** (0.138) (0.244) (0.233) (0.243) lnpop 1.171*** 0.806*** 0.929*** 1.153*** (0.097) (0.213) (0.195) (0.293) gattwto08 0.006 0.007 0.004 0.0003 (0.007) (0.006) (0.006) (0.004) lnmcap08 0.399** 0.286* 0.114 (0.172) (0.167) (0.237) rintr -0.010 -0.009 -0.007 (0.010) (0.010) (0.009) topint08 -0.051*** -0.058*** -0.060*** (0.011) (0.012) (0.015) nrrents -0.005 0.013 (0.010) (0.013) roflaw 0.203 0.342 (0.372) (0.283) fullprivproc -0.002* (0.001) Constant -29.050*** -19.444*** -20.858*** -25.951*** (2.578) (4.820) (4.255) (6.240) Observations 197 131 131 113 Log Likelihood -438.540 -259.731 -256.024 -179.661 Akaike Inf. Crit. 885.079 533.461 530.049 379.322 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 3.3 Example II - Structural estimation In this section the goal is to estimate the parameters from likelihood function of a given model, and be able to calculate it in the statistical software (in this case we are using R). We are going to estimate the structural parameters of a very simple search model (Flinn and Heckman 1982), following Flinn and Heckman 1982. This tutorial is not devoted to understanding such labor model, so we are going to describe the the model and its assumptions; from that point we are going to derivate the ML function. Then we are going to pass that function to the computer as in the previous case, and maximize it to find the parameters of the model. 3.3.0.1 The model: Agents are well behaved, and try to maximize it’s utility maximizing the income they receive. When they are in unemployment, they face a cost \\(c\\) for searching. When they face it, offers from a Poisson process will arrive. The arrival offer rate is denoted by \\(\\lambda\\), and it’s probability by unit of time is also \\(\\lambda\\). When they meet a wage is proposed from a log-normal distribution and the individuals can refuse to form the match or ‘seal the deal’. Another assumption is that the length of the employment spells follow a exponential distribution, and there is a constant risk of loosing the job at a rate \\(\\eta\\). Time is discounted at a rate \\(\\rho\\). Such model is characterized by two bellman equations. The first equation is the value of being employed [qe. (3.2) of the paper]: \\[ V_e(w) = \\frac{1}{\\rho}[w + (1-\\eta)V_e + \\eta V_u]\\] Which reorganizing terms is equal to: \\[V_e(w) = \\frac{1}{\\rho + \\eta}[w + \\eta V_u ]\\] The second equation is the value of being unemployed: \\[V_u = \\frac{1}{\\rho}[c+(1-\\lambda)V_u+ \\lambda + \\mathbf{E} \\max \\lbrace V_e, V_u \\rbrace]\\] Which, after rearranging is equal to: \\[\\rho V_u = -c + \\frac{\\lambda}{\\rho + \\eta} \\int_{\\rho V_u}(w-\\rho V_u)f(w)\\] These two equations describe the whole behaviour of the economy under the assumptions of the model. Now let’s list all the assumptions that we have untill now, since they are going to be usefull for the construction of the likelihhod function: From the model we have that the minimum accepted wage is equal to \\(w^* = \\rho V_u\\). In the paper the minimum accepted wage is not estimated, but instead is taken from the data. Arrival rate of offers: \\(\\lambda\\). Termination rate: \\(\\eta\\) Distribution of employment duration. \\(f[t_e]= \\eta e^{(-\\eta t_e)}\\); the average duration is then \\(\\mathbf{E}[t_e]= \\eta^{-1}\\). Rate of leaving the unemployed is equal to: \\(\\lambda (1- F(w^*))\\). We aare going to call this \\(h_u\\) Distribution of unemployment duration is exponential \\(f[t_u]= h_u e^{-h_u t_u)}\\). The expected value in unemployment then is then equal to \\((\\lambda (1- F(w^*)))^{-1}=h_u^{-1}\\) The distribution of accepted wages is equal to: \\(\\frac{f(w)}{1 - F(w^*)}\\). The term below is to adjust it to a distribution since a proper distribution must integrate to 1 and our is left censored. The distribution of employments spells is right censored. Given that is negative exponential it coincide with the population. Given all this information we can proceed to calculate the probability to sample an employed individual, and the probability to sample an unemployed individual. Let’s start first by the probability of sample an unemployed indivdual: \\[P(U)=\\frac{\\mathbf{E}[t_u]}{\\mathbf{E}[t_u]+\\mathbf{E}[t_e]}=\\frac{h_u^{-1}}{h_u^{-1}+\\eta^{-1}}=\\frac{\\eta}{h_u+\\eta}\\] Now let’s state the probability of sample and employed individual: \\[P(E)=\\frac{\\mathbf{E}[t_e]}{\\mathbf{E}[t_u]+\\mathbf{E}[t_e]}=\\frac{\\eta^{-1}}{h_u^{-1}+\\eta^{-1}}=\\frac{h_u}{h_u+\\eta}\\] Given that we have normally data on the duration of unemployment \\(t_u^o\\) and the wages \\(w^o\\) observed from census or surveys, we can calculate such parameters because now we can define the likelihood: \\[ \\begin{equation} \\begin{aligned} \\mathcal{L}&amp;=\\prod_U [P(U) \\times f(t_u^o)]\\times \\prod_E [P(E) \\times f(w^o)]=\\\\ &amp;= \\prod_U\\left[ P(U) \\times \\lambda (1- F(w^*)) e^{- \\lambda (1- F(w^*)) t_u} \\right] \\times \\prod_E \\left[P(E) \\times \\frac{f(w|\\mu, \\sigma)}{1 - F(w^*)} \\right]=\\\\ &amp;= \\prod_U\\left[\\frac{\\eta}{h_u+\\eta} \\times \\lambda (1- F(w^*)) e^{- \\lambda (1- F(w^*)) t_u} \\right] \\times \\prod_E \\left[\\frac{h_u}{h_u+\\eta} \\times \\frac{f(w|\\mu, \\sigma)}{1 - F(w^*)} \\right]=\\\\ &amp;= \\prod_U\\left[\\frac{\\eta}{\\lambda (1- F(w^*))+\\eta} \\times \\lambda (1- F(w^*)) e^{- \\lambda (1- F(w^*)) t_u} \\right] \\times\\\\ &amp;\\prod_E \\left[\\frac{\\lambda (1- F(w^*))}{\\lambda (1- F(w^*))+\\eta} \\times \\frac{f(w|\\mu, \\sigma)}{1 - F(w^*)} \\right]\\\\ \\end{aligned} \\end{equation} \\] After taking logs and rearranging terms we arrive to the following log-likelihood: \\[\\log \\mathcal{L} = N \\log(h_u) - h_u \\sum t_u + N_u \\log(\\eta) + \\sum f(w|\\mu,\\sigma) - N_e \\log(1-F(w^*)) - N_e \\log(h_u + \\eta)\\] We just need to put this function into the optimizer with some data and we can obtain the parameters of the model. Before continuing to implement the likelihood function in R we are going to make a sop and gather some data to estimate our model. 3.3.0.2 The Data In order to estimate the model we just need two vectors of data: duration of unemployment for the unemployed and hourly wages for the employed. To make a real example we are going to use the Current population survey (CSP), but look that most of the household surveys contain these kind of data (i.e. colombian GEIH). First we go to the page of the CPS, and learn about the nature of the data. We can also download the historical monthly data from the NBER webpage. For this exercise we are going to use the January 2019 data, that can be optained following this link. Download and unzip the dataset in your working directory. Take also a moment to check the required variables, looking the documentation for the data. The open the dataset using the functionality of the reader package (Wickham, Hester, and Francois 2017). data &lt;- read_csv(&quot;jan19pub.dat&quot;, col_names = FALSE, trim_ws = FALSE) As you will see the data only contains 1 vector of text. Each row is a long sring of text. If you took the time of reading the documentation you will see that you can fin next to each variable the description and the location of the specific data in such long string. After reading the documentation we identify that in order to have a correct vector of wages for the employed and a correct vector of duration of the unemployed we are going to need the following information: GROUP OF INTEREST VARIABLE LENGTH DESCRIPTION LOCATION EMPLOYED HRMIS 2 MONTH-IN-SAMPLE 63 - 64 EMPLOYED PEERNPER 2 PERIODICITY 502 - 503 EMPLOYED PEERNHRO 2 USUAL HOURS 525 - 526 EMPLOYED PRERNWA 8 WEEKLY EARNINGS RECODE 527 - 534 EMPLOYED PRERNHLY 4 RECODE FOR HOURLY RATE 520 - 523 EMPLOYED PEHRUSL1 2 HOW MANY HOURS PER WEEK DO YOU WORK 218 - 219 EMPLOYED PTWK 1 WEEKLY EARNINGS - TOP CODE 535 - 535 UNEMPLOYED PEMLR 2 MONTHLY LABOR FORCE RECODE 180 - 181 UNEMPLOYED RUNEDUR 3 DURATION OF UNEMPLOYMENT FOR 407 - 409 ALL GESTFIPS 2 FEDERAL INFORMATION STATE 93 - 94 After identifying this information we proceed to filter the information which is necessary. In order to do so we are going to create a database for the employed and a database for the unemployed given that the identification involves different infromation and variables. Let’s begin with employed: we create a ‘data.frame’ containing the relevant colums and we extract the information by the position in the string. employed &lt;- data.frame(matrix(NA, nrow = nrow(data), ncol = 8)) colnames(employed) &lt;- c( &quot;HRMIS&quot;, &quot;PEERNPER&quot;, &quot;PEERNHRO&quot;, &quot;PRERNWA&quot;, &quot;PRERNHLY&quot;, &quot;PTWK&quot;, &quot;PEHRUSL1&quot;, &quot;GESTFIPS&quot;) employed$HRMIS &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),63,64))) employed$PEERNPER &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),502,503))) employed$PEERNHRO &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),525,526))) employed$PRERNWA &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),527,534))) employed$PRERNHLY &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),520,523))) employed$PTWK &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),535,535))) employed$PEHRUSL1 &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),218,219))) employed$GESTFIPS &lt;- apply(data,1, function(x) substr(toString(x),93,94)) Reading the documentation we identify that the invalid cases are coded \\(0\\) or negative values \\(-1,-2,\\dots\\). We reclassify this coded infromation as NA, a not available or missing value. We are going to keep the observations that are from the outgoing rotations (have earnings information). After that we are going to filter the valid hourly waes and convert the weekkly waes to hours using the number of hours. We are going to keep only the people that has this information. We are also going to take the right number of decimal for the variables which specify it. We are going to keep only the observations that are above the legal federal minimum wage of \\(7,5\\) USD, and we are going to trim the data at the percentile 99.9%. employed[employed&lt;=0] &lt;- NA employed &lt;- employed[which(employed$HRMIS %in% c(4,8)),] employed &lt;- employed[which(employed$PEERNPER &gt; 0),] employed &lt;- employed[-which(employed$PRERNHLY == 9999),] employed$PRERNHLY &lt;- employed$PRERNHLY/100 employed &lt;- employed[-which(employed$PTWK == 1),] employed$PRERNWA &lt;- employed$PRERNWA/100 employed$wages &lt;- ifelse(employed$PRERNHLY &gt; 0 &amp; !is.na(employed$PRERNHLY), employed$PRERNHLY, ifelse(!is.na(employed$PRERNWA) &amp; !is.na(employed$PEHRUSL1), employed$PRERNWA/employed$PEHRUSL1, NA) ) employed &lt;- employed[which(!is.na(employed$wages)),] employed &lt;- employed[which(employed$wages &gt;= 7.25),] employed &lt;- employed[which(employed$wages &lt;= quantile(employed$wages, 0.999)),] tokeep_e &lt;- c(&quot;GESTFIPS&quot;, &quot;wages&quot;) employed &lt;- employed[,tokeep_e] employed$duration_U &lt;- 0 Now we proceed to do the same for the unemployed: We collect the infromation in each variable following the position. We take the people that the labor employment status is unemployment and that has information on the duration. Then we convert the information to montly data. unemployed &lt;- data.frame(matrix(NA, nrow = nrow(data), ncol = 3)) colnames(unemployed) &lt;- c(&quot;PEMLR&quot;, &quot;RUNEDUR&quot;, &quot;GESTFIPS&quot;) unemployed$PEMLR &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),180,181))) unemployed$RUNEDUR &lt;- apply(data,1, function(x) as.numeric(substr(toString(x),407,409))) unemployed$GESTFIPS &lt;- apply(data,1, function(x) substr(toString(x),93,94)) unemployed &lt;- unemployed[which(unemployed$PEMLR %in% c(3,4)),] unemployed &lt;- unemployed[which(unemployed$RUNEDUR &gt; 0),] unemployed$duration_U &lt;- unemployed$RUNEDUR/4.333 unemployed &lt;- unemployed[,c(&quot;GESTFIPS&quot;,&quot;duration_U&quot;)] unemployed$wages &lt;- 0 unemployed &lt;- unemployed[,c(&quot;GESTFIPS&quot;,&quot;wages&quot;, &quot;duration_U&quot;)] We the attach the two data frames: data &lt;- rbind(employed,unemployed) We are going to select and calculate the MLE using only each state. For this exercise we are going to use the information on Nevada (\\(GESTFIPS = 32\\)) data_sub &lt;- data[which(data$GESTFIPS==&quot;32&quot;),] 3.3.0.3 Estimation In order to estimate the identified log likelihood we are going to code two auxiliary funcions that appear all the time in the procedure. The first one is the log-normal density function: \\[\\phi \\left( \\frac{\\log(x) - \\mu}{\\sigma} \\right)(\\frac{1}{x \\sigma})\\] Where \\(\\phi\\) is the probability density function of the \\(N(0,1)\\) distribution. lognorm &lt;- function(x,mu,sigma){ res &lt;- dnorm((log(x)-mu)/(sigma))*(1/(sigma*x)) return(res) } The second function is the survival function, which is the value of the probability to be over the minimum accepted wage given a distribution. We define the survival as \\((1- F(w^*))\\). The cumulative distribution function of the log-normal is equalt to: \\[\\Phi \\left( \\frac{\\log(x) - \\mu}{\\sigma} \\right)\\] Where \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution. surv &lt;- function(val,mu,sigma){ res &lt;- 1 - pnorm((log(val)-mu)/(sigma)) return(res) } There are other functions already developed that are suitable for this kind of problems. One xample is the mle2 function from the ‘bbmle’ package (Bolker and Team 2017). This is useful since the function deals with standard errors and provide other information that might be usefull. There are some difference between the optim() method already covered and the mle2() function. This function requires: Function to calculate negative log-likelihood A start list for the optimizer The optimizer used Given that we are going to introcue the data as an input of the function, we need the data Now we are going to code the log-likelihood function we had recovered using the same procedure as in the previous example: First we are going to code each of the parameters as an input in the function so the function can recognize it, and as mentioned earlier also the data. Then we are going to write the likelihood using the definition derived earlier. LLfnct_mle &lt;- function(alambda,aeta,amu,asigma, data){ lambda = exp(alambda) eta = exp(aeta) mu = amu sigma = exp(asigma) w_star &lt;- min(data[which(data$wages &gt; 0),]$wages) h_u &lt;- lambda * surv(w_star, mu, sigma) n &lt;- nrow(data) n_u &lt;- nrow(data[which(data$duration_U &gt; 0),]) n_e &lt;- (n-n_u) LL &lt;- n * log(h_u) - n_e * log(surv(w_star, mu, sigma)) - h_u * sum(data$duration_U) + n_u * log(eta) + sum(log(lognorm(data[which(data$wages&gt;0),]$wages,mu,sigma))) - n_e * log(eta + h_u) #print(cbind(c(&quot;lambda = &quot;,&quot;eta = &quot;,&quot;mu = &quot;,&quot;sigma = &quot;),c(lambda,eta,mu,sigma))) return(-LL) } After that we just have to set up the maximizer and maximize the function: m0 &lt;- mle2(LLfnct_mle, start = list(alambda =-1, aeta = -1, amu = 3, asigma = -1), data = list(data = data_sub), optimizer = &quot;nlminb&quot;) m0@coef ## alambda aeta amu asigma ## -0.4541099 -1.8175680 2.7146674 -0.4361533 All the coeficients need to be transformed so we recover them and present the results. But before that we are going to calculate the standard error using the delta method and the information from the hessian. At the end what we are doing is just rescaling the errors. lambda &lt;- exp(m0@coef[&quot;alambda&quot;]) eta &lt;- exp(m0@coef[&quot;aeta&quot;]) mu &lt;- m0@coef[&quot;amu&quot;] sigma &lt;- exp(m0@coef[&quot;asigma&quot;]) fisher_info &lt;- m0@details$hessian vcov_mle &lt;- solve(fisher_info) prop_sigma&lt;-sqrt(c(lambda^2,eta^2,mu^2,sigma^2) * diag(vcov_mle)) names(prop_sigma) &lt;- c(&quot;lambda&quot;,&quot;eta&quot;,&quot;mu&quot;,&quot;sigma&quot;) stargazer(cbind(&quot;parameter&quot; = names(prop_sigma), &quot;theta&quot; =c(lambda,eta,mu,sigma), prop_sigma), title = &quot;Model parameters MLE - Nevada (Jan 2019)&quot;, type=ifelse(knitr::is_latex_output(),&quot;latex&quot;,&quot;html&quot;), out = &quot;./images/SE_Nevada.tex&quot;) (#tab:)Model parameters MLE - Nevada (Jan 2019) parameter theta prop_sigma alambda lambda 0.635012931441372 0.0900528154150777 aeta eta 0.16242027479781 0.0379917018868457 amu mu 2.71466743713899 0.270516421768113 asigma sigma 0.646518617780906 0.0678137201168286 3.4 References This material was possible thanks to the slides of David MARGOLIS (in PSE resources), the course of C. FLinn at CCA 2017, and QuantEcon MLE. "]
]
